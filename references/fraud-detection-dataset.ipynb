{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79929c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:21:12.314963Z",
     "iopub.status.busy": "2025-06-28T08:21:12.314682Z",
     "iopub.status.idle": "2025-06-28T08:21:16.180013Z",
     "shell.execute_reply": "2025-06-28T08:21:16.178776Z"
    },
    "papermill": {
     "duration": 3.87739,
     "end_time": "2025-06-28T08:21:16.181348",
     "exception": true,
     "start_time": "2025-06-28T08:21:12.303958",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.utils._metadata_requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/3846559228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from . import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.utils._metadata_requests'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd8d11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CSV_URL = \"/kaggle/input/fraud-detection-in-transactions-dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6f60a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbeb33",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871e838",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the shape of the dataset (number of rows and columns)\n",
    "print(f'Dataset contains {df.shape[0]} rows and {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752d809",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display concise summary of the DataFrame, including data types and non-null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761d96c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate descriptive statistics of the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e55490",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd845c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## --- Exploratory Data Analysis (EDA) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797563b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Class Distribution of 'label' (Fraud vs. Not Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156ba66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5b970",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = ['Not Fraud (0)', 'Fraud (1)']\n",
    "colors = ['lightgreen', 'salmon']\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(label_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, explode=[0, 0.1])\n",
    "plt.title('Fraud vs Not Fraud Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc3225",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Analysis of 'amount' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a99e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, and IQR for outlier detection\n",
    "Q1 = df['amount'].quantile(0.25)\n",
    "Q3 = df['amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds for outlier detection using the IQR method\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers in the 'amount' column\n",
    "outliers = df[(df['amount'] < lower_bound) | (df['amount'] > upper_bound)]\n",
    "print(f\"Number of outliers in 'amount': {len(outliers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43c5ba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize the distribution and outliers of transaction amounts\n",
    "sns.boxplot(data=df, x='amount')\n",
    "plt.title('Boxplot of Transaction Amounts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b24f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a histogram to visualize the distribution of transaction amounts\n",
    "sns.histplot(df['amount'], bins=50, kde=True)\n",
    "plt.title('Transaction Amount Distribution')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0694b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### The amount column is highly skewed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5bdfe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Fraud Rate by Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031af22b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define categorical features for analysis\n",
    "cat_features = ['merchant_type', 'device_type']\n",
    "\n",
    "# Loop through each categorical feature to visualize fraud rate\n",
    "for col in cat_features:\n",
    "    fraud_rate = df.groupby(col)['label'].mean().sort_values(ascending=False)\n",
    "    fraud_rate.plot(kind='bar')\n",
    "    plt.title(f'Fraud Rate by {col}')\n",
    "    plt.ylabel('Fraud Rate')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9553c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Fraud Rate by Transaction Amount Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55051b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for categorizing transaction amounts into ranges\n",
    "bins = [0, 50, 100, 200, 400, 800, df['amount'].max()]\n",
    "labels = ['0-50', '50-100', '100-200', '200-400', '400-800', '800+']\n",
    "\n",
    "# Create a new column 'amount_range' by binning the 'amount' column\n",
    "df['amount_range'] = pd.cut(df['amount'], bins=bins, labels=labels)\n",
    "\n",
    "# Calculate the mean fraud rate for each 'amount_range'\n",
    "fraud_rate_by_amount = df.groupby('amount_range', observed=True)['label'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ce981",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Fraud rate wrt to transaction amount range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4e417",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the fraud rate by transaction amount range\n",
    "fraud_rate_by_amount.plot(kind='bar', color='teal')\n",
    "plt.title('Fraud Rate by Transaction Amount Range')\n",
    "plt.xlabel('Amount Range')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36c86f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 🧠 Exploratory Data Analysis (EDA) – Key Insights & Interpretations\n",
    "\n",
    "### ✅ Dataset Summary:\n",
    "- **Rows**: 1000  \n",
    "- **Features**: 5  \n",
    "- **Target (`label`)**: Binary (0 = non-fraud, 1 = fraud)  \n",
    "- **Missing Values**: None\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Class Distribution:\n",
    "- **Non-Fraud (label = 0)**: 950  \n",
    "- **Fraud (label = 1)**: 50  \n",
    "- ⚠️ **Highly imbalanced** (~5% fraud rate)\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Fraud Rate by `amount` Range:\n",
    "\n",
    "| Amount Range | Fraud Rate |\n",
    "|--------------|------------|\n",
    "| 100–200      | **~8%** 🔺 |\n",
    "| 400–800      | **~6%** 🔺 |\n",
    "| Other ranges | < 5%       |\n",
    "\n",
    "**Interpretation:**  \n",
    "Fraud tends to concentrate in **mid to upper ranges of transaction amounts**, likely because:\n",
    "- Low amounts aren’t attractive enough for fraud\n",
    "- Extremely high amounts may trigger additional security\n",
    "\n",
    "---\n",
    "\n",
    "### 🛍️ Fraud Rate by `merchant_type`:\n",
    "\n",
    "| Merchant Type | Fraud Rate |\n",
    "|---------------|------------|\n",
    "| Others        | **~7%** 🔺 |\n",
    "| Electronics   | ~5%        |\n",
    "| Others (groceries, travel, clothing) | < 4% |\n",
    "\n",
    "**Interpretation:**  \n",
    "The \"others\" category could include high-risk or uncategorized vendors. Electronics is also slightly riskier — potentially due to resale value.\n",
    "\n",
    "---\n",
    "\n",
    "### 📱 Fraud Rate by `device_type`:\n",
    "\n",
    "| Device Type | Fraud Rate |\n",
    "|-------------|------------|\n",
    "| Mobile      | **~5%**    |\n",
    "| Tablet      | ~5%        |\n",
    "| Desktop     | ~4%        |\n",
    "\n",
    "**Interpretation:**  \n",
    "Slightly more frauds occur on **mobile and tablet devices**, possibly due to easier spoofing or less secure access compared to desktop.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Actionable Insights:\n",
    "\n",
    "- `amount`, `merchant_type`, and `device_type` show useful variance with fraud label — ✅ relevant for modeling.\n",
    "- Handling class imbalance during modeling via `class_weight` or `SMOTE`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabd681",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## --- Data Preprocessing ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb1719",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Transformation of 'amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76f14a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply log1p transformation to the 'amount' column to reduce skewness\n",
    "df['log_amount'] = np.log1p(df['amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bfd3e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Add a binary feature for mid-to-high risk amount range\n",
    "# df['is_mid_high_amount'] = ((df['amount'] > 100) & (df['amount'] < 800)).astype(int)\n",
    "# This technique didn't improve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17c2d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the log-transformed 'amount'\n",
    "sns.histplot(df['log_amount'], bins=50, kde=True)\n",
    "plt.title('Transaction log Amount Distribution')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7320af7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The skewness of the original and log-transformed 'amount'\n",
    "print(\"Original skew:\", df['amount'].skew())\n",
    "print(\"Log skew:\", df['log_amount'].skew())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57d24f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Scaling 'log_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4788f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to the 'log_amount' column\n",
    "df['log_amount_scaled'] = scaler.fit_transform(df[['log_amount']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea056b42",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Feature Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2adb629",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop 'transaction_id' as it's an identifier and 'amount_range' which was used for EDA\n",
    "df = df.drop(columns=['transaction_id', 'amount_range']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91eab3d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### One-Hot Encoding for Categorical Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b9bbb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply One-Hot Encoding to 'merchant_type' and 'device_type'\n",
    "# 'drop_first=True' prevents multicollinearity\n",
    "df_encoded = pd.get_dummies(df,columns=['merchant_type', 'device_type'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bc419",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the head of the encoded DataFrame\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c649f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1058bca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot a correlation heatmap to visualize relationships between features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_encoded.corr(),annot=True,cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90f1b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## --- Model Training and Evaluation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b1865",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to prepare features (X) and target (y)\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Separates the DataFrame into features (X) and target (y).\n",
    "    Drops the original 'amount' and 'log_amount' columns from features.\n",
    "    \"\"\"\n",
    "    X = df.drop(['amount', 'log_amount', 'label'], axis=1)\n",
    "    y = df['label']\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bbe2c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function for hyperparameter tuning using GridSearchCV\n",
    "def tune_model(model, param_grid, X_train, y_train, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning on a given model using GridSearchCV.\n",
    "    Evaluates models based on the 'f1' score.\n",
    "    \"\"\"\n",
    "    grid = GridSearchCV(model, param_grid, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model_name}: {grid.best_params_}\")\n",
    "    return grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebe05d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function for stratified train-test splitting\n",
    "def stratified_split(X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets while preserving\n",
    "    the proportion of classes in the target variable (stratified split).\n",
    "    \"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949753be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to apply SMOTE for handling class imbalance\n",
    "def apply_smote(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Applies Synthetic Minority Over-sampling Technique (SMOTE) to the\n",
    "    training data to address class imbalance.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    return smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ca589",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to evaluate a given model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a classification model using various metrics including\n",
    "    Accuracy, Precision, Recall, F1-score, AUC, Confusion Matrix, and ROC Curve.\n",
    "    Handles both scikit-learn models and Keras models (for ANN).\n",
    "    \"\"\"\n",
    "    # Predict probabilities or decision function scores\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):  # For SVC with probability=True\n",
    "        y_proba = model.decision_function(X_test)\n",
    "        # Normalize decision scores to [0, 1] for AUC if not already probabilities\n",
    "        y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min())\n",
    "    else:  # Assume Keras model with direct probability output\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "    # Convert probabilities to binary predictions based on a 0.5 threshold\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'AUC': roc_auc_score(y_test, y_proba),\n",
    "        'ConfusionMatrix': confusion_matrix(y_test, y_pred),\n",
    "        'ROC': roc_curve(y_test, y_proba),\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0496d7c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborn.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {title}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3360ca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to plot the ROC curve\n",
    "def plot_roc_curve(fpr, tpr, model_name, auc_score):\n",
    "    \"\"\"\n",
    "    Plots the Receiver Operating Characteristic (ROC) curve.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")  # Diagonal dashed line for random classifier\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve - {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499db011",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to run and evaluate multiple classification models\n",
    "def run_all_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a set of common classification models.\n",
    "    Prints metrics and plots confusion matrices and ROC curves for each.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'KNN': KNeighborsClassifier(), # KNN does not support class_weight directly, SMOTE will help\n",
    "        'SVM': SVC(probability=True, class_weight='balanced', random_state=42),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        results[name] = metrics\n",
    "\n",
    "        print(f\"\\n🔍 {name} Evaluation:\")\n",
    "        for k, v in metrics.items():\n",
    "            if k not in ['ConfusionMatrix', 'ROC']:\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "        plot_confusion_matrix(metrics['ConfusionMatrix'], name)\n",
    "        fpr, tpr, _ = metrics['ROC']\n",
    "        plot_roc_curve(fpr, tpr, name, metrics['AUC'])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d646372",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- Main execution flow for model training and evaluation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bf417",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y) from the encoded DataFrame\n",
    "X, y = prepare_features(df_encoded)\n",
    "\n",
    "# Split the data into training and testing sets using stratified sampling\n",
    "X_train, X_test, y_train, y_test = stratified_split(X, y)\n",
    "\n",
    "# Apply SMOTE to the training data to balance the classes\n",
    "X_train_sm, y_train_sm = apply_smote(X_train, y_train)\n",
    "\n",
    "# Run and evaluate all defined classification models\n",
    "results = run_all_models(X_train_sm, X_test, y_train_sm, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbc924",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566064f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune Random Forest Classifier\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "best_rf = tune_model(rf, rf_param_grid, X_train_sm, y_train_sm, model_name=\"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f046f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune Support Vector Machine (SVM) Classifier\n",
    "svm = SVC(probability=True, class_weight='balanced', random_state=42)\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "best_svm = tune_model(svm, svm_param_grid, X_train_sm, y_train_sm, model_name=\"SVM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473099d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Evaluate Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f250934",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate and plot results for the best (tuned) Random Forest and SVM models\n",
    "for name, model in [('Tuned Random Forest', best_rf), ('Tuned SVM', best_svm)]:\n",
    "    print(f\"\\n--- Evaluating {name} ---\")\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    print(f\"\\n🔧 {name} Evaluation:\")\n",
    "    for k, v in metrics.items():\n",
    "        if k not in ['ConfusionMatrix', 'ROC']:\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "    plot_confusion_matrix(metrics['ConfusionMatrix'], name)\n",
    "    fpr, tpr, _ = metrics['ROC']\n",
    "    plot_roc_curve(fpr, tpr, name, metrics['AUC'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c57ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## --- Artificial Neural Network (ANN) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461470d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame to NumPy arrays for Keras\n",
    "X_train_ann = X_train_sm.to_numpy()\n",
    "X_test_ann = X_test.to_numpy()\n",
    "y_train_ann = y_train_sm.to_numpy()\n",
    "y_test_ann = y_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0a255",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the ANN model architecture\n",
    "ann = Sequential([\n",
    "    Dense(32, input_dim=X_train_ann.shape[1], activation='relu'), # Input layer with 32 neurons, ReLU activation\n",
    "    Dropout(0.3), # Dropout layer to prevent overfitting\n",
    "    Dense(16, activation='relu'), # Hidden layer with 16 neurons, ReLU activation\n",
    "    Dense(1, activation='sigmoid') # Output layer with 1 neuron (binary classification), Sigmoid activation\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2f483",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the ANN model\n",
    "ann.compile(\n",
    "    optimizer='adam', # Adam optimizer\n",
    "    loss='binary_crossentropy', # Binary cross-entropy loss for binary classification\n",
    "    metrics=['accuracy'] # Monitor accuracy during training\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4cbe4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Early Stopping callback to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef13e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the ANN model\n",
    "print(\"\\n--- Training Artificial Neural Network (ANN) ---\")\n",
    "history = ann.fit(\n",
    "    X_train_ann, y_train_ann,\n",
    "    validation_split=0.2, # Use 20% of training data for validation\n",
    "    epochs=50, # Maximum number of epochs\n",
    "    batch_size=32, # Batch size for training\n",
    "    callbacks=[early_stop], # Apply early stopping\n",
    "    verbose=1 # Show training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4432f99",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predicted probabilities from the ANN model\n",
    "y_pred_prob_ann = ann.predict(X_test_ann).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6097d6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert probabilities to binary predictions using a 0.5 threshold\n",
    "y_pred_ann = (y_pred_prob_ann >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eddd79",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-define evaluate_model function to correctly handle Keras model output\n",
    "# This is necessary if the previous evaluate_model was not designed for Keras\n",
    "# If the previous evaluate_model was already handling Keras, this re-definition might be redundant.\n",
    "# However, it's safer to ensure correct handling of y_pred and y_proba.\n",
    "def evaluate_model_keras(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a Keras classification model specifically.\n",
    "    Calculates various metrics and returns them.\n",
    "    \"\"\"\n",
    "    y_proba = model.predict(X_test).flatten()\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1': f1_score(y_test, y_pred),\n",
    "        'AUC': roc_auc_score(y_test, y_proba),\n",
    "        'ConfusionMatrix': confusion_matrix(y_test, y_pred),\n",
    "        'ROC': roc_curve(y_test, y_proba),\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db1449",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the ANN model\n",
    "print(\"\\n--- ANN Evaluation ---\")\n",
    "metrics_ann = evaluate_model_keras(ann, X_test_ann, y_test_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa426c44",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print ANN evaluation metrics\n",
    "for k, v in metrics_ann.items():\n",
    "    if k not in ['ConfusionMatrix', 'ROC']:\n",
    "        print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94f1f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix for ANN\n",
    "plot_confusion_matrix(metrics_ann['ConfusionMatrix'], \"ANN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78445da",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot ROC Curve for ANN\n",
    "fpr, tpr, _ = metrics_ann['ROC']\n",
    "plot_roc_curve(fpr, tpr, \"ANN\", metrics_ann['AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132af1a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Model Performance & Conclusion\n",
    "\n",
    "This section summarizes the performance of various classification models on the fraud detection task, highlighting challenges and offering insights for future improvements.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation Metrics Overview\n",
    "\n",
    "The following table presents key evaluation metrics for all trained models, including those after hyperparameter tuning.\n",
    "\n",
    "> ⚠️ **Note**: Due to the highly imbalanced nature of the dataset (95% non-fraud, 5% fraud), **Accuracy** is a misleading metric. Instead, **Precision**, **Recall**, **F1-Score**, and **AUC** are critical for assessing fraud detection effectiveness.\n",
    "\n",
    "| Model                 | Accuracy | Precision | Recall | F1-Score | AUC    |\n",
    "|----------------------|----------|-----------|--------|----------|--------|\n",
    "| Logistic Regression  | 0.4200   | 0.0431    | 0.5000 | 0.0794   | 0.4100 |\n",
    "| Decision Tree        | 0.6850   | 0.0656    | 0.4000 | 0.1127   | 0.5500 |\n",
    "| Random Forest        | 0.6900   | 0.0667    | 0.4000 | 0.1143   | 0.4589 |\n",
    "| KNN                  | 0.6550   | 0.0462    | 0.3000 | 0.0800   | 0.4024 |\n",
    "| SVM                  | 0.6000   | 0.0139    | 0.1000 | 0.0244   | 0.4295 |\n",
    "| Tuned Random Forest  | 0.6550   | 0.0462    | 0.3000 | 0.0800   | 0.4650 |\n",
    "| Tuned SVM            | 0.6150   | 0.0282    | 0.2000 | 0.0494   | 0.3937 |\n",
    "| ANN                  | 0.9500   | 0.0000    | 0.0000 | 0.0000   | 0.4416 |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis of Model Performance & Why Models Struggled\n",
    "\n",
    "### Dominant Impact of Class Imbalance\n",
    "\n",
    "- Despite using techniques like **SMOTE** and `class_weight='balanced'`, the **extreme imbalance** (95% non-fraud vs. 5% fraud) severely affected test performance.\n",
    "- Models show **very low F1-Scores**, reflecting poor trade-offs between precision and recall for fraud cases.\n",
    "- The **ANN's high accuracy (0.95)** but zero precision and recall is a classic sign of a model always predicting the majority class, missing fraud completely.\n",
    "- This underscores why **accuracy is a poor metric** for imbalanced datasets.\n",
    "\n",
    "### Insufficiently Discriminative Features\n",
    "\n",
    "- The primary limitation lies in the lack of **strong predictive signals** in the current features.\n",
    "\n",
    "#### `log_amount_scaled` Distribution Overlap\n",
    "\n",
    "- Visualizations revealed **significant overlap** between fraud and non-fraud transactions.\n",
    "- Even after transformation, transaction amount doesn't serve as a clear signal.\n",
    "\n",
    "#### Limited Categorical Feature Impact\n",
    "\n",
    "- Features like `merchant_type` and `device_type` showed **minor fraud rate differences** during EDA.\n",
    "- The \"Others\" category in `merchant_type` had a higher fraud rate but lacked specificity.\n",
    "\n",
    "### Limitations of Current Approach\n",
    "\n",
    "- **SMOTE**: Creates synthetic samples from existing minority class points. If these points are not informative, synthetic data can be **noisy** or lead to **overfitting**.\n",
    "- **Model Limitations**: Both simple (Logistic Regression, SVM-linear) and complex models (Random Forest, ANN, RBF-SVM) failed, suggesting the issue is **data quality**, not model choice.\n",
    "\n",
    "---\n",
    "\n",
    "## Future Recommendations for Improvement\n",
    "\n",
    "To improve fraud detection, the focus should shift toward **richer, more discriminative features**.\n",
    "\n",
    "### Rich Feature Engineering\n",
    "\n",
    "- **Time-Based Features** (if timestamp available):\n",
    "  - `time_of_day`, `day_of_week`, `transaction_frequency_per_user`\n",
    "\n",
    "- **Behavioral Features**:\n",
    "  - Average transaction amount over time windows (e.g., last 1hr, 24hr)\n",
    "  - Number of unique merchants/devices used recently\n",
    "  - Ratio of current amount to user’s historical average\n",
    "  - Indicators of new or unusual locations\n",
    "\n",
    "### External Data Integration\n",
    "\n",
    "- Use **blacklists**, or public data about **high-risk merchant categories**.\n",
    "\n",
    "### Anomaly Detection Techniques\n",
    "\n",
    "- Shift from supervised learning to:\n",
    "  - **Isolation Forest**\n",
    "  - **One-Class SVM**\n",
    "  - **Autoencoders**\n",
    "- Better suited for **rare outliers** like fraud in imbalanced data.\n",
    "\n",
    "### Deep Dive into Feature Interactions\n",
    "\n",
    "- Explore **feature combinations** or interactions that may highlight subtle fraud patterns not visible in isolation.\n",
    "\n",
    "---\n",
    "\n",
    "> By focusing on feature quality and exploring alternative detection strategies, there is significant potential to improve fraud detection on challenging, imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe0bbf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- For Numerical Feature: log_amount_scaled ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_encoded[df_encoded['label'] == 0]['log_amount_scaled'], color='blue', label='Non-Fraud (0)', kde=True, stat='density', alpha=0.6, common_norm=False)\n",
    "sns.histplot(df_encoded[df_encoded['label'] == 1]['log_amount_scaled'], color='red', label='Fraud (1)', kde=True, stat='density', alpha=0.6, common_norm=False)\n",
    "plt.title('Distribution of log_amount_scaled by Fraud Status')\n",
    "plt.xlabel('Scaled Log Amount')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8573fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7018088,
     "sourceId": 11234331,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.037569,
   "end_time": "2025-06-28T08:21:16.814050",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-28T08:21:07.776481",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
